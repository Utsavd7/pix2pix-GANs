# pix2pix: Image-to-image translation

_**1) Overview**_

1. This repository illustrates the process of constructing and training a pix2pix conditional generative adversarial network (cGAN). 
2. The objective of this network, as outlined in the paper "Image-to-image translation with conditional adversarial networks", is to learn how to convert input images into corresponding output images.
3. The versatility of pix2pix allows it to be employed for various tasks such as producing photos from label maps, adding color to grayscale images, converting Google Maps images into aerial views, and changing sketches into realistic photographs.

_**2) Architecture**_

The architecture of your network will contain:

1. A generator with a U-Net-based architecture.
2. A discriminator represented by a convolutional PatchGAN classifier (proposed in the pix2pix paper).
3. Note that each epoch can take around 15 seconds on a single V100 GPU.

Below are some examples of the output generated by the pix2pix cGAN after training for 200 epochs on the facades dataset (80k steps).

![1](https://github.com/Utsavd7/pix2pix-GANs/assets/46219693/296259af-1631-42f0-807e-adbb340b3cd3)
